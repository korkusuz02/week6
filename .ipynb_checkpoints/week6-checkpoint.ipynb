{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dabd8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dask import dataframe as dd\n",
    "import csv\n",
    "import gzip\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5bed5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramDosyalaari\\anaconda\\lib\\site-packages\\pandas\\util\\__init__.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing\n"
     ]
    }
   ],
   "source": [
    "veri = pd.DataFrame(data=np.random.randint(99999, 99999999, size=(10000000,14)),columns=['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14'])\n",
    "veri['C15'] = pd.util.testing.rands_array(5,10000000)\n",
    "veri.to_csv(\"new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "veri = pd.read_csv('new_data.csv')\n",
    "end = time.time()\n",
    "print(\"loads the whole CSV file at once in the memory in a single dataframe: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7230a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#read data in chunks of 1 million rows at a time\n",
    "veri = pd.read_csv('new_data.csv',chunksize=1000000)\n",
    "end = time.time()\n",
    "print(\"Instead of reading the whole CSV at once, chunks of CSV are read into memory: \",(end-start),\"sec\")\n",
    "pd_df = pd.concat(veri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ef967",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "veri = pd.read_csv('new_data.csv',delimiter=',')\n",
    "end = time.time()\n",
    "print(\"Read csv with pandas: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "veri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16032c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile testutility.py\n",
    "\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.load(stream, Loader=yaml.Loader)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n",
    "\n",
    "def col_header_val(df,table_config):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace('[^\\w]','_',regex=True)\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))\n",
    "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
    "    expected_col.sort()\n",
    "    df.columns =list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
    "        print(\"column name and column length validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"column name and column length validation failed\")\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        print(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
    "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
    "        print(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
    "        logging.info(f'df columns: {df.columns}')\n",
    "        logging.info(f'expected columns: {expected_col}')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae56fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile testutility.py\n",
    "import yaml\n",
    "import logging\n",
    "import re\n",
    "\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.load(stream, Loader=yaml.Loader)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n",
    "\n",
    "def replacer(string, char):\n",
    "    pattern = char + '{2,}'\n",
    "    string = re.sub(pattern, char, string)\n",
    "    return string\n",
    "\n",
    "def col_header_val(df, table_cfg):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace('[^\\w]', '_', regex=True)\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "    df.columns = list(map(lambda x: replacer(x, '_'), list(df.columns)))\n",
    "    expected_col = list(map(lambda x: x.lower(), table_cfg[\"columns\"]))\n",
    "    expected_col.sort()\n",
    "    #df.columns = list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    if list(expected_col) == list(df.columns):\n",
    "        print(\"column name validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"column name validation failed\")\n",
    "        mismatch = list(set(df.columns).difference(expected_col))\n",
    "        print(\"Columns not in YAML file: \", mismatch)\n",
    "        missing =  list(set(expected_col).difference(df.columns))\n",
    "        print(\"Columns not in data file: \", missing)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile numericdata.yaml\n",
    "file_type: csv\n",
    "dataset_name: file\n",
    "file_name: new_data\n",
    "inbound_delimiter: \",\"\n",
    "outbound_delimiter: \"|\"\n",
    "skip_leading_rows: 1\n",
    "columns: \n",
    "      - C1\n",
    "      - C2\n",
    "      - C3\n",
    "      - C4\n",
    "      - C5\n",
    "      - C6\n",
    "      - C7\n",
    "      - C8\n",
    "      - C9\n",
    "      - C10\n",
    "      - C11\n",
    "      - C12\n",
    "      - C13\n",
    "      - C14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417cbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config file\n",
    "import testutility as util\n",
    "config_data = util.read_config_file(\"numericdata.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data['inbound_delimiter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ad12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfddf1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(\"new_data.csv\",delimiter=',')\n",
    "df_sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc80ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file using config file\n",
    "file_type = config_data['file_type']\n",
    "source_file = \"./\" + config_data['file_name'] + f'.{file_type}'\n",
    "#print(\"\",source_file)\n",
    "df = pd.read_csv(source_file,config_data['inbound_delimiter'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output gz file\n",
    "df.to_csv('new_datas.csv.gz',\n",
    "          sep='|',\n",
    "          header=True,\n",
    "          index=False,\n",
    "          quoting=csv.QUOTE_ALL,\n",
    "          compression='gzip',\n",
    "          quotechar='\"',\n",
    "          doublequote=True,\n",
    "          line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Size of the file\n",
    "os.path.getsize('new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of the file\n",
    "os.path.getsize('new_datas.csv.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
